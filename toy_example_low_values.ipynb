{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780566fc-fa7a-42e2-b5fa-17f47f09493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from KL_optimization import alternating_kl_projection\n",
    "from Beta_optimization import alternating_beta_projection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e7d88-70dc-4c7a-9d41-dea567d6c190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between p and empirical distribution\n",
    "    \n",
    "    Parameters:\n",
    "        p: distribution, array_like\n",
    "        beta: distribution, array_like, same dimensions as p\n",
    "    \n",
    "    Returns:\n",
    "        KL divergence between p and q, scalar\n",
    "    \"\"\"\n",
    "    return np.sum(p * np.log((p) / (q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893475c-2a96-4eea-b4a2-3e4381ef64b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_empirical(true_pi, num_samples):\n",
    "    \"\"\"\n",
    "    Construct the empirical distribution from the sample from discrete distribution \n",
    "    \n",
    "    Parameters:\n",
    "        true_pi: distribution, array_like\n",
    "        num_samples: sample size, scalar\n",
    "    \n",
    "    Returns:\n",
    "        empirical distribution, array_like, same dimensions as true_pi\n",
    "        \n",
    "    \"\"\"\n",
    "    # Flatten the true distribution\n",
    "    flat_probs = true_pi.flatten()\n",
    "    \n",
    "    # Generation of empirical distribution from the true distribution\n",
    "    picks = np.random.choice(len(flat_probs), size=num_samples, p=flat_probs)\n",
    "    emp   = np.zeros_like(flat_probs)\n",
    "    uniq, cnts = np.unique(picks, return_counts=True)\n",
    "    emp[uniq] = cnts / num_samples\n",
    "    \n",
    "    # Reshape back to the original shape\n",
    "    return emp.reshape(true_pi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325afc5-d602-4181-b249-9e8f265d484e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_matrix(alpha, beta=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    Sample a 3×2  matrix x for pmf such that:\n",
    "      x[0,0] = alpha\n",
    "      all entries >= beta\n",
    "\n",
    "    Parameters:\n",
    "        alpha : Fixed value for x[0,0]. Must satisfy 0 <= alpha <= 1 - 5*beta, scalar            \n",
    "        beta : Minimum value for all other entries, scalar, default=0.1\n",
    "        seed : Random seed for reproducibility, int or None\n",
    "\n",
    "    Returns:\n",
    "        x : pmf, array of shape (3,2)\n",
    "        p : marginal distribution over rows, array of shape 3\n",
    "        q : marginal distribution over columns, array of shape 2\n",
    "\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Validate inputs\n",
    "    n_slots = 3*2 - 1  # total entries minus the fixed one\n",
    "    if not (0 <= alpha <= 1 - n_slots * beta):\n",
    "        raise ValueError(f\"alpha must be in [0, {1 - n_slots * beta}], got {alpha}\")\n",
    "\n",
    "    # Initialize matrix and reserve the first entry\n",
    "    x = np.empty((3, 2), dtype=float)\n",
    "    x[0, 0] = alpha\n",
    "    remaining_budget = 1.0 - alpha\n",
    "\n",
    "    # Indices of the remaining entries, in an order that respects future bounds\n",
    "    fill_order = [(1, 0), (0, 1), (2, 0), (1, 1), (2, 1)]\n",
    "    slots_left = len(fill_order)\n",
    "\n",
    "    # Fill all but the last entry\n",
    "    for (i, j) in fill_order[:-1]:\n",
    "        max_val = remaining_budget - (slots_left - 1) * beta\n",
    "        if max_val < beta:\n",
    "            raise ValueError(\n",
    "                f\"Not enough budget to allocate at least {beta} to each of the {slots_left} slots; \"\n",
    "                f\"remaining_budget={remaining_budget:.4f}, beta={beta}\")\n",
    "        x[i, j] = np.random.uniform(beta, max_val)\n",
    "        remaining_budget -= x[i, j]\n",
    "        slots_left -= 1\n",
    "\n",
    "    # Last entry takes all remaining budget\n",
    "    last_i, last_j = fill_order[-1]\n",
    "    if remaining_budget < beta:\n",
    "        raise ValueError(\n",
    "            f\"Remaining budget {remaining_budget:.4f} is below beta {beta}\")\n",
    "    x[last_i, last_j] = remaining_budget\n",
    "\n",
    "    # Compute marginals\n",
    "    p = x.sum(axis=1)  # row sums\n",
    "    q = x.sum(axis=0)  # column sums\n",
    "    return x, p, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c3400-a817-45f8-9466-68398af0b12d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "target_trials = 1000\n",
    "num_samples   = 50\n",
    "num_matrices  = 1\n",
    "alpha         = 0.01\n",
    "beta_sampling = 0.1\n",
    "\n",
    "# epsilon grid\n",
    "eps_range = np.linspace(0.001, 0.01, num=10)\n",
    "\n",
    "# beta-projection parameters to compare\n",
    "beta_list = [1.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4c84e-85a4-413e-9aa2-21c17b0127dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample true distributions\n",
    "matrix_samples = [sample_matrix(alpha, beta_sampling, seed=i) for i in range(num_matrices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb95aa7-48ae-4296-9475-a56f02ac91bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating projections, keeping track of three statistics:\n",
    "# Mean of [0,0] element;\n",
    "# Amount of zeros in projection distribution\n",
    "# KL divergence value between projected and true distribution\n",
    "for idx, (true_pi, marg1, marg2) in enumerate(matrix_samples, start=1):\n",
    "    print(f\"\\n=== Matrix {idx}/{num_matrices} ===\")\n",
    "    print(\"True matrix (true_pi):\")\n",
    "    print(true_pi)\n",
    "\n",
    "    # Stats for all projections\n",
    "    stats = {\n",
    "        # Stats for KL\n",
    "        'kl_mean00': [],\n",
    "        'kl_zero00': [],\n",
    "        'kl_meandiv': [],\n",
    "        # Stats for Beta for each beta in list\n",
    "        'beta_mean00': {b: [] for b in beta_list},\n",
    "        'beta_zero00': {b: [] for b in beta_list},\n",
    "        'beta_meandiv': {b: [] for b in beta_list},\n",
    "    }\n",
    "\n",
    "    for eps in eps_range:\n",
    "        # Output for KL\n",
    "        kl_vals00 = []\n",
    "        kl_divs   = []\n",
    "        kl_zeros  = 0\n",
    "\n",
    "        # Output for Beta\n",
    "        beta_vals00   = {b: [] for b in beta_list}\n",
    "        beta_divs     = {b: [] for b in beta_list}\n",
    "        beta_zeros    = {b: 0 for b in beta_list}\n",
    "\n",
    "        for _ in range(target_trials):\n",
    "            # Generation of empirical distribution from the sample\n",
    "            pi_hat = sample_empirical(true_pi, num_samples)\n",
    "\n",
    "            # KL projection\n",
    "            proj_kl = alternating_kl_projection(\n",
    "                pi_hat, marginals=[marg1, marg2], num_iters=100,\n",
    "                replace_zeros=True, epsilon=eps\n",
    "            )\n",
    "            kl_vals00.append(proj_kl[0, 0])\n",
    "            kl_divs.append(kl_divergence(proj_kl, true_pi))\n",
    "            kl_zeros += (proj_kl[0, 0] == 0)\n",
    "\n",
    "            # Beta projections for each beta in list\n",
    "            for b in beta_list:\n",
    "                proj_bt = alternating_beta_projection(\n",
    "                    pi_hat, marginals=[marg1, marg2], beta=b,\n",
    "                    num_iters=1, full_cycles=50,\n",
    "                    replace_zeros=True, epsilon=eps\n",
    "                )\n",
    "                beta_vals00[b].append(proj_bt[0, 0])\n",
    "                beta_divs[b].append(kl_divergence(proj_bt, true_pi))\n",
    "                beta_zeros[b] += (proj_bt[0, 0] == 0)\n",
    "\n",
    "        # Update KL stats\n",
    "        stats['kl_mean00'].append(np.mean(kl_vals00))\n",
    "        stats['kl_zero00'].append(kl_zeros)\n",
    "        stats['kl_meandiv'].append(np.mean(kl_divs))\n",
    "\n",
    "        # Update Beta stats\n",
    "        for b in beta_list:\n",
    "            stats['beta_mean00'][b].append(np.mean(beta_vals00[b]))\n",
    "            stats['beta_zero00'][b].append(beta_zeros[b])\n",
    "            stats['beta_meandiv'][b].append(np.mean(beta_divs[b]))\n",
    "\n",
    "        # Print stats for each iteration\n",
    "        summary = (\n",
    "            f\"eps={eps:.4f} | KL: mean00={stats['kl_mean00'][-1]:.6f}, zeros={stats['kl_zero00'][-1]}, \"\n",
    "            f\"div={stats['kl_meandiv'][-1]:.6f}\"\n",
    "        )\n",
    "        for b in beta_list:\n",
    "            summary += (\n",
    "                f\" || β={b}: mean00={stats['beta_mean00'][b][-1]:.6f}, \"\n",
    "                f\"zeros={stats['beta_zero00'][b][-1]}, \"\n",
    "                f\"div={stats['beta_meandiv'][b][-1]:.6f}\"\n",
    "            )\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661e21a-5a80-4d27-903e-b7930a754977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean [0,0] comparison (two curves only) and save in current directory\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(eps_range, stats['kl_mean00'], 'o-', label='KL', color='blue')\n",
    "b = beta_list[0]\n",
    "plt.plot(eps_range, stats['beta_mean00'][b], marker='s', linestyle='-', label=rf'$\\beta={b}$', color='orange')\n",
    "plt.xlabel(r'Imputation for $\\hat\\pi_{00}$')\n",
    "plt.ylabel(r'$\\mathbb{E}[x_{00}]$')\n",
    "plt.title(r'The optimal value of $x_{00}$ against inputted value $\\hat\\pi_{00}$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mean_x00_vs_pi_hat00_low.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c6d2e-2ee7-4434-8610-dba56806c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean KL divergence comparison (two curves only) and save in current directory\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(eps_range, stats['kl_meandiv'], 'o-', label='KL', color='blue')\n",
    "plt.plot(eps_range, stats['beta_meandiv'][b], marker='s', linestyle='-', label=rf'$\\beta={b}$', color='orange')\n",
    "plt.xlabel(r'Imputation for $\\hat\\pi_{00}$')\n",
    "plt.ylabel(r'Mean KL divergence')\n",
    "plt.title(r'Mean KL divergence of $x$ vs inputted value $\\hat\\pi_{00}$')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('mean_KL_vs_pi_hat00_low.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8965a-0582-4e3e-b8b5-a8e35995d062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
